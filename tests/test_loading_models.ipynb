{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db64cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f966a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20af7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb362875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ab60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3():\n",
    "    \n",
    "    def __init__(self, heu_explore=False, role=['retailer'], target_inv=None, state_dim=16, action_dim=2):\n",
    "        \n",
    "        self.heu_explore = heu_explore\n",
    "#         self.env = build_beer_game_uniform(player=role)\n",
    "        self.env = build_beer_game_uniform_multi_player(players=role)\n",
    "        \n",
    "        self.ad = ActionDecoder(action_space, action_type='y')\n",
    "        self.noise = ExplorationActionMod(action_space, target_inv=target_inv)\n",
    "\n",
    "#         state_dim  = 8\n",
    "#         action_dim = 1\n",
    "        \n",
    "#         state_dim  = 16\n",
    "#         action_dim = 2\n",
    "        \n",
    "        \n",
    "        hidden_dim = 256\n",
    "\n",
    "        self.value_net1 = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.value_net2 = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "        self.target_value_net1 = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_value_net2 = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "        soft_update(self.value_net1, self.target_value_net1, soft_tau=1.0)\n",
    "        soft_update(self.value_net2, self.target_value_net2, soft_tau=1.0)\n",
    "        soft_update(self.policy_net, self.target_policy_net, soft_tau=1.0)\n",
    "\n",
    "\n",
    "        self.value_criterion = nn.MSELoss()\n",
    "\n",
    "        policy_lr = 3e-4\n",
    "        value_lr  = 3e-4\n",
    "\n",
    "        self.value_optimizer1 = optim.Adam(self.value_net1.parameters(), lr=value_lr)\n",
    "        self.value_optimizer2 = optim.Adam(self.value_net2.parameters(), lr=value_lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "\n",
    "        replay_buffer_size = 2 ** 20\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "        \n",
    "        self.episode_idx = 0\n",
    "        self.reward_history = []\n",
    "        self.training_history = []\n",
    "        \n",
    "        \n",
    "    def td3_update(self, \n",
    "                   step,\n",
    "                   batch_size,\n",
    "                   gamma=0.95,\n",
    "                   soft_tau=1e-2,\n",
    "                   noise_std=0.2 * 1,\n",
    "                   noise_clip=0.5 * 1,\n",
    "                   policy_update=2,\n",
    "                  ):\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        state      = torch.FloatTensor(state).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        action     = torch.FloatTensor(action).to(device)\n",
    "        reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "        done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "        next_action = self.target_policy_net(next_state)\n",
    "        noise = torch.normal(torch.zeros(next_action.size()), noise_std).to(device)\n",
    "        noise = torch.clamp(noise, -noise_clip, noise_clip)\n",
    "        next_action += noise\n",
    "\n",
    "        target_q_value1  = self.target_value_net1(next_state, next_action)\n",
    "        target_q_value2  = self.target_value_net2(next_state, next_action)\n",
    "        target_q_value   = torch.min(target_q_value1, target_q_value2)\n",
    "        expected_q_value = reward + (1.0 - done) * gamma * target_q_value\n",
    "\n",
    "        q_value1 = self.value_net1(state, action)\n",
    "        q_value2 = self.value_net2(state, action)\n",
    "\n",
    "        value_loss1 = self.value_criterion(q_value1, expected_q_value.detach())\n",
    "        value_loss2 = self.value_criterion(q_value2, expected_q_value.detach())\n",
    "\n",
    "        self.value_optimizer1.zero_grad()\n",
    "        value_loss1.backward()\n",
    "        self.value_optimizer1.step()\n",
    "\n",
    "        self.value_optimizer2.zero_grad()\n",
    "        value_loss2.backward()\n",
    "        self.value_optimizer2.step()\n",
    "\n",
    "        if step % policy_update == 0:\n",
    "            policy_loss = self.value_net1(state, self.policy_net(state))\n",
    "            policy_loss = -policy_loss.mean()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            soft_update(self.value_net1, self.target_value_net1, soft_tau=soft_tau)\n",
    "            soft_update(self.value_net2, self.target_value_net2, soft_tau=soft_tau)\n",
    "            soft_update(self.policy_net, self.target_policy_net, soft_tau=soft_tau)\n",
    "\n",
    "        return value_loss1.item(), value_loss2.item()\n",
    "    \n",
    "    \n",
    "    def train(self, run, episode=1000):\n",
    "        for epi in tqdm(range(episode)):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            epi_vl1, epi_vl2 = 0, 0\n",
    "            actions = []\n",
    "\n",
    "            for step in count():\n",
    "                action = self.policy_net.get_action(state)\n",
    "\n",
    "                # add noise to the action during training\n",
    "                action = self.noise.add_noise(action, state, self.episode_idx, exploration_strategy='gaussian', heu_explore=self.heu_explore)\n",
    "                \n",
    "#                 print(action)\n",
    "                quantity = self.ad.decode(action, state)\n",
    "                \n",
    "#                 print(quantity)\n",
    "                next_state, reward, done, _ = self.env.step(quantity)\n",
    "\n",
    "                actions.append(action)\n",
    "\n",
    "                self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        #         replay_buffer.push(state, quantity, reward, next_state, done)\n",
    "\n",
    "\n",
    "                if len(self.replay_buffer) > batch_size:\n",
    "                    step_vl1, step_vl2 = self.td3_update(step, batch_size)\n",
    "                    epi_vl1 += step_vl1\n",
    "                    epi_vl2 += step_vl2\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                if done:\n",
    "                    self.episode_idx += 1\n",
    "                    self.training_history.append([run, self.episode_idx, self.heu_explore, episode_reward, epi_vl1/step, epi_vl2/step])\n",
    "                    break\n",
    "\n",
    "\n",
    "            if self.episode_idx % 10 == 0:\n",
    "                for i in range(5):\n",
    "                    \n",
    "                    self.reward_history.append(self.test(run))\n",
    "\n",
    "\n",
    "#             if self.episode_idx % 15 == 0:\n",
    "#                 plot_test()\n",
    "\n",
    "    def test(self, run):\n",
    "\n",
    "#         cul_rewards = []\n",
    "\n",
    "        state = self.env.reset()\n",
    "\n",
    "        cul_reward = 0\n",
    "        for t in count():\n",
    "            action = self.policy_net.get_action(state)\n",
    "#             quantity = int(self.ad.decode(action, state))\n",
    "            quantity = self.ad.decode(action, state)\n",
    "#             print(quantity)\n",
    "            next_state, reward, done, _ = self.env.step(quantity)\n",
    "\n",
    "            cul_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return [run, self.heu_explore, self.episode_idx, cul_reward]\n",
    "#         cul_rewards.append(cul_reward)\n",
    "\n",
    "#         self.reward_history.append(np.mean(cul_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff6fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDecoder():\n",
    "    def __init__(self, action_space, action_type='y'):\n",
    "        self.low  = action_space['low']\n",
    "        self.high = action_space['high']\n",
    "        \n",
    "        self.action_type = action_type\n",
    "    \n",
    "    def decode(self, action, state):\n",
    "        quantity = self.low + (np.array(action) + 1.0) * 0.5 * (self.high - self.low)\n",
    "        quantity = np.clip(quantity, self.low, self.high)\n",
    "\n",
    "        if self.action_type == 'd+y':  # TODO\n",
    "#             quantity = max(0, (state[2] + quantity)[0])\n",
    "            quantity = np.maximum(0, [state[i*8+2] + quantity[i] for i in range(quantity.size)])\n",
    "        elif self.action_type == 'y':\n",
    "#             quantity = quantity[0] + 8.0\n",
    "            quantity = quantity + 8.0\n",
    "\n",
    "        return np.round(quantity, decimals=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e52805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplorationActionMod(object):\n",
    "    def __init__(self, action_space, max_sigma=0.9, min_sigma=0.1, decay_episode=1000, target_inv=None):\n",
    "        self.low  = action_space['low']\n",
    "        self.high = action_space['high']\n",
    "        \n",
    "        self.max_sigma = max_sigma\n",
    "        self.min_sigma = min_sigma\n",
    "        self.decay_episode = decay_episode\n",
    "        \n",
    "        self.target_inv = target_inv\n",
    "    \n",
    "    def add_noise(self, action, state, t=0, exploration_strategy='gaussian', heu_explore=False):\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if exploration_strategy == 'gaussian':\n",
    "            # sigma linearly decays from max to min \n",
    "            sigma  = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_episode)\n",
    "            action = action + np.random.normal(size=len(action)) * sigma\n",
    "        \n",
    "        elif exploration_strategy == 'epsilon':\n",
    "#             EPS = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_episode)\n",
    "            if sample < EPS:\n",
    "                action = [random.random()*2-1]\n",
    "            else:\n",
    "                action = action + np.zeros(1)\n",
    "                \n",
    "               \n",
    "        if heu_explore:\n",
    "            assert self.target_inv is not None\n",
    "            \n",
    "#         EPS = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_episode)\n",
    "        EPS = self.max_sigma - self.max_sigma * min(1.0, t / self.decay_episode)\n",
    "        sample = random.random()        \n",
    "        \n",
    "        if heu_explore == True:\n",
    "#             if sample < EPS ** 3 * 0.05:\n",
    "#             if sample < EPS ** 3:\n",
    "            if sample < (EPS/2) ** 2:\n",
    "                # exploration guided by heuristics\n",
    "#                 self.target_inv = 19 # // TODO\n",
    "#                 bsq = self.target_inv - (state[0] + state[3] - state[1])\n",
    "    \n",
    "                bsq = np.array(self.target_inv) - np.array([state[i*8+0] + state[i*8+3] - state[i*8+1] for i in range(self.target_inv.size)])\n",
    "\n",
    "    #             if self.action_type == 'd+y':\n",
    "    #                 hea = np.clip((bsq - state[2]).cpu().numpy(), -8, 8) + 8\n",
    "    #             elif self.action_type == 'y':\n",
    "#                 hea = (np.clip(bsq, 2, 18) - 2 ) / 8.0 - 1.0\n",
    "\n",
    "                # if action type is d+y \n",
    "                d = np.array([state[2]]+[state[(i-1)*8+5] for i in range(1,self.target_inv.size)])\n",
    "                unfilled_demand = d + [state[i*8+1] for i in range(self.target_inv.size)]\n",
    "                unfilled_demand[0] = state[1] \n",
    "                \n",
    "                bsq = np.array(self.target_inv) - np.array([state[i*8+0] + state[i*8+3] - unfilled_demand[i] for i in range(self.target_inv.size)])\n",
    "\n",
    "                    \n",
    "#                 d = np.array([state[2]]+[state[(i-1)*8+6] for i in range(1,self.target_inv.size)])\n",
    "        \n",
    "                hea = np.clip(bsq-d, -8, 8) / 8.0\n",
    "#                 action = np.array([hea])\n",
    "                action = np.array(hea)\n",
    "\n",
    "\n",
    "        \n",
    "#         return np.clip(action, self.low, self.high)\n",
    "        return np.clip(action, -1.0, 1.0)\n",
    "    \n",
    "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/gaussian_strategy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1884d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilized = [i for i in range(32)]\n",
    "# utilized.remove(26)\n",
    "# utilized.remove(18)\n",
    "# utilized.remove(10)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "#         state = state[:, utilized]\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "#         state = state[:, utilized]\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1beda729",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_single_player_manufacturer.pickle', 'rb') as f:\n",
    "    loadedmodels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc6db59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1248.14\n"
     ]
    }
   ],
   "source": [
    "r = 0\n",
    "runs = 100\n",
    "\n",
    "for i in range(runs):\n",
    "    result = loadedmodels[0].test(1000)\n",
    "    r += result[3]\n",
    "    \n",
    "print(r/runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611cea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
